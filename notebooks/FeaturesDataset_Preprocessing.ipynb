{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import folium\n",
        "import selenium\n",
        "import h5py\n",
        "\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ahUGRFhColMb"
      },
      "outputs": [],
      "source": [
        "all_features = pd.read_pickle(r'../data/all_features.pkl')\n",
        "all_features = all_features.sort_values(by='mesh_id').reset_index(drop=True)\n",
        "all_features = all_features[['mesh_id', 'latitude', 'longitude', 'area', 'prefecture']]\n",
        "all_features = all_features.drop_duplicates(subset=['mesh_id'], keep='first')\n",
        "\n",
        "# reports_selected = pd.read_csv(r'../data/reports_selected.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "lat_width = 75 / 9000\n",
        "lon_height = 0.0125\n",
        "\n",
        "bottom_lat = 35.7015082\n",
        "bottom_lon = 139.5221197"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient(img):\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    imgf = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n",
        "    return imgf\n",
        "\n",
        "def erosion(img):\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    imgf = cv2.erode(img, kernel, iterations=1)\n",
        "    return imgf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "gxUz-NZ46XRw",
        "outputId": "592cf4c5-7e2f-492a-95b4-d70975de019b"
      },
      "outputs": [],
      "source": [
        "def preprocessed_image_from_coords(lat, lon, resize=True):\n",
        "    # -------------------\n",
        "    # Creating folium map\n",
        "    # -------------------\n",
        "\n",
        "    north = lat + lat_width\n",
        "    south = lat\n",
        "    east = lon + lon_height\n",
        "    west = lon\n",
        "\n",
        "    def mean(x, y):\n",
        "        return (x + y) / 2\n",
        "\n",
        "    map_obj = folium.Map(location=[\n",
        "        mean(north, south), mean(east, west)], tiles='Cartodb dark_matter no_labels')\n",
        "\n",
        "    square_coordinates = [\n",
        "        (south, west),\n",
        "        (north, west),\n",
        "        (north, east),\n",
        "        (south, east),\n",
        "        (south, west)\n",
        "    ]\n",
        "\n",
        "    folium.PolyLine(\n",
        "        locations=square_coordinates,\n",
        "        weight=7,\n",
        "    ).add_to(map_obj)\n",
        "\n",
        "    map_obj.fit_bounds([(south, west), (north, east)])\n",
        "\n",
        "    # ------------------------\n",
        "    # Save folium map to image\n",
        "    # ------------------------\n",
        "\n",
        "    timeout = 1\n",
        "\n",
        "    img_data = map_obj._to_png(timeout)\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # ---------------------------\n",
        "    # Detect blue box around mesh\n",
        "    # ---------------------------\n",
        "\n",
        "    image_np = np.array(img)\n",
        "    border_thickness = 8\n",
        "\n",
        "    if image_np.shape[-1] == 4:  # Handle RGBA\n",
        "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2BGR)\n",
        "    else:\n",
        "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    hsv = cv2.cvtColor(image_np, cv2.COLOR_BGR2HSV)\n",
        "    lower_blue = np.array([100, 150, 50])\n",
        "    upper_blue = np.array([140, 255, 255])\n",
        "\n",
        "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # As for right now, just log the details and \n",
        "        # manually add in the images into the dataset later\n",
        "\n",
        "        date_string = datetime.now().strftime('%Y-%m-%d_%H-%M-%s')\n",
        "        with open(fr'../logs/{date_string}.txt', 'w+') as f:\n",
        "            f.write(f'Latitude: {lat}\\n')\n",
        "            f.write(f'Longitude: {lon}\\n')\n",
        "\n",
        "        return np.zeros((256, 256))\n",
        "        \n",
        "        # raise ValueError(f\"No blue box detected in the mesh at {lat}, {lon}\")\n",
        "\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "\n",
        "    x_min = x + border_thickness\n",
        "    y_min = y + border_thickness\n",
        "    x_max = x + w - border_thickness\n",
        "    y_max = y + h - border_thickness\n",
        "\n",
        "    y_min, y_max = min(y_min, y_max), max(y_min, y_max)\n",
        "    x_min, x_max = min(x_min, x_max), max(x_min, x_max)\n",
        "\n",
        "    cropped_np = image_np[y_min:y_max, x_min:x_max]\n",
        "    cv2_image = cv2.cvtColor(cropped_np, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    cv2_image_copy = cv2_image.copy()\n",
        "\n",
        "    lab = cv2.cvtColor(cv2_image_copy, cv2.COLOR_BGR2LAB)\n",
        "    l_channel, a, b = cv2.split(lab)\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=15.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l_channel)\n",
        "\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "\n",
        "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "    enhanced_img = gradient(enhanced_img)\n",
        "\n",
        "    # --------------------------\n",
        "    # Apply Canny edge detection\n",
        "    # --------------------------\n",
        "\n",
        "    edges = cv2.Canny(enhanced_img, 50, 150)\n",
        "\n",
        "    if resize:\n",
        "        standard_size = (256, 256)\n",
        "        resized_edges = cv2.resize(edges, standard_size, interpolation=cv2.INTER_AREA)\n",
        "    else:\n",
        "        resized_edges = edges\n",
        "\n",
        "    return resized_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(row):\n",
        "    latitude, longitude = row.latitude, row.longitude\n",
        "    return preprocessed_image_from_coords(latitude, longitude)\n",
        "\n",
        "def create_hdf5_dataset(df, file_keyword, dataset_name='features', target_shape=(256, 256), batch_size=100):\n",
        "    num_samples = len(df)\n",
        "    start_idx = 0\n",
        "\n",
        "    hdf5_filename = fr'../datasets/{file_keyword}.h5'\n",
        "\n",
        "    if os.path.exists(hdf5_filename):\n",
        "        with h5py.File(hdf5_filename, 'r+') as hdf5_file:\n",
        "            dataset = hdf5_file[dataset_name]\n",
        "\n",
        "            start_idx = np.sum(dataset[:] != 0) // target_shape[0]\n",
        "            print(f\"Resuming from index {start_idx}\")\n",
        "\n",
        "    else:\n",
        "        with h5py.File(hdf5_filename, 'w') as hdf5_file:\n",
        "            hdf5_file.create_dataset(\n",
        "                dataset_name,\n",
        "                shape=(num_samples, *target_shape),\n",
        "                dtype=np.uint8,\n",
        "                compression='gzip',\n",
        "                compression_opts=9\n",
        "            )\n",
        "\n",
        "    with h5py.File(hdf5_filename, 'r+') as hdf5_file:\n",
        "        dataset = hdf5_file[dataset_name]\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            for batch_start in tqdm(range(start_idx, num_samples, batch_size), desc=\"Processing Batches\"):\n",
        "                batch_end = min(batch_start + batch_size, num_samples)\n",
        "                batch_rows = df.iloc[batch_start:batch_end]\n",
        "\n",
        "                batch_results = list(executor.map(process_row, batch_rows.itertuples(index=False)))\n",
        "\n",
        "                dataset[batch_start:batch_end] = np.array(batch_results, dtype=np.uint8)\n",
        "\n",
        "    print(f\"Features dataset saved as {hdf5_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Batches:  42%|████▏     | 64/153 [2:13:56<3:27:50, 140.12s/it]"
          ]
        }
      ],
      "source": [
        "create_hdf5_dataset(\n",
        "    all_features,\n",
        "    file_keyword='features1_kanto',\n",
        "    dataset_name='images',\n",
        "    target_shape=(256, 256),\n",
        "    batch_size=100\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "traffic-accident-analysis-K9TuC2qz",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
